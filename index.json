[
{
	"uri": "https://rombix92.github.io/blog/encyklopedia/1_r_markdown/",
	"title": "R_Markdown",
	"tags": [],
	"description": "Presentation of R Markdown functionality",
	"content": "  Markdown Tutorial for markdown: https://bookdown.org/yihui/rmarkdown/\nBelow some useful examples:\nknitr is what runs each code chunk and knits the document. Knitr will use this option as default for each cchunk in the document when the file is knit. Followiw opts_chunk$dollar_set() we can add the options that we want to set globally to the parentheses before the echo = TRUE argument\n include - code \u0026amp; results appears in the result echo - code appear in the knit file eval - evaluate code in a code chunk collapse - split code and any text output into multiple blocks or include in a single block in the final report warning - display warning message - display message like from loading packages error - stop kniting file when the error will occure (if false file will knit anyway)  At the begining of the markdown document I can find YAML header.\nThe YAML header controls the look and feel of your document. At the very least, your R markdown document should contain the following YAML header sandwiched between two sets of ---:\n --- title: \u0026quot;Your document title\u0026quot; author: \u0026quot;ES 218\u0026quot; output: html_document: default ---  "
},
{
	"uri": "https://rombix92.github.io/blog/encyklopedia/2_data_grapling/",
	"title": "Data Exploration",
	"tags": [],
	"description": "Methods for data explorations",
	"content": "  Data Grapling EDA 1.Take a look at data 2.Visualize Data 3.Count basic statistics\nOne quick technique for jump-starting EDA is to examine all of the pairwise scatterplots in your data. This can be achieved using the pairs() function. Look for variables in the nyc data set that are strongly correlated, as those relationships will help us check for multicollinearity later on.\ndf_it_restaurants \u0026lt;- read.csv(\u0026#39;https://assets.datacamp.com/production/repositories/845/datasets/639a7a3f9020edb51bcbc4bfdb7b71cbd8b9a70e/nyc.csv\u0026#39;) pairs(df_it_restaurants %\u0026gt;% select(-Restaurant))  Missing Data NA = Not Available NaN = Not a Number\nlibrary(\u0026#39;datasets\u0026#39;) df_airquality \u0026lt;- airquality NA|TRUE ## [1] TRUE NA|FALSE ## [1] NA NA+NaN ## [1] NA NaN+NA ## [1] NA library(naniar)   "
},
{
	"uri": "https://rombix92.github.io/blog/encyklopedia/3_statistics/",
	"title": "Statistics",
	"tags": [],
	"description": "Describing statistical methods",
	"content": "   Logistic Regression  Matematyczna interpretacja modelu Graficzna interpretacja modelu  Bayesian Statistics - Introduction  Introduction Priors Contrasts and comparison Dealing with 2 parameter model Automatisation - BEST package Conclusions  Bayesian Statistics - Intermediate  Likelihood Posterior    Logistic Regression Markdown Tutorial\nMatematyczna interpretacja modelu Quiz correct answers: d. Hint: Remember, the coefficient in a logistic regression model is the expected increase in the log odds given a one unit increase in the explanatory variable.\n  Survived Fare Fare_log    0 7.2500 2  1 71.2833 4  1 7.9250 2  1 53.1000 4  0 8.0500 2  0 8.4583 2    Wyliczanie modelu logistycznego.\nmodel \u0026lt;- glm(data=df_titanic, Survived ~ Fare_log, family = \u0026#39;binomial\u0026#39;) tidy(model) %\u0026gt;% kable(caption=\u0026#39;Table 1. Summary statistics for logistic regression model\u0026#39;)  Table 1: Table 1. Summary statistics for logistic regression model  term estimate std.error statistic p.value    (Intercept) -2.3337286 0.2452271 -9.516601 0  Fare_log 0.6442428 0.0792358 8.130706 0    Model wyliczany jest zgodnie z ponizsza formula dlategp by otrzymac oszacowania paraemtrow w formie ich wpływu na odds musimy je poddać działaniu exp()\ncoef(model) ## (Intercept) Fare_log ## -2.3337286 0.6442428 #Tak przemnozone wspolczynniki interpretujemy nastepujaco: # o ile % wzrosnie odds wystapienia zdarzenia jezeli wzrosnie nam wartosc predyktora o 1 exp(coef(model)) ## (Intercept) Fare_log ## 0.09693365 1.90454431 Ponizej w sposob matematyczny pokazuje ze to wlasnie oznacza interpretacja wzrostu parametra stajacego przy predyktorze.\ndf_aug \u0026lt;- augment(model, type.predict = \u0026quot;response\u0026quot;) # without response argument, the fitted value will be on log-odds scale p3 = df_aug$.fitted[df_aug$Fare_log==3][1] p2 = df_aug$.fitted[df_aug$Fare_log==2][1] x \u0026lt;- round(p3/(1-p3)/(p2/(1-p2)),5) # i sprawdzenie czy dobrze rozumiem zależnosc x1\u0026lt;-round(exp(coef(model))[\u0026#39;Fare_log\u0026#39;],5) x1==x ## Fare_log ## TRUE Prob for Fare_log = 2 was equal to 0.2601396 while for Fare_log = 3 was equal to 0.401072. The odds increase by 1.90454. The same what model results suggests -\u0026gt; 1.90454.\nQuiz\nThe fitted coefficient from the medical school logistic regression model is 5.45. The exponential of this is 233.73.\nDonald’s GPA is 2.9, and thus the model predicts that the probability of him getting into medical school is 3.26%. The odds of Donald getting into medical school are 0.0337, or—phrased in gambling terms—29.6:1. If Donald hacks the school’s registrar and changes his GPA to 3.9, then which of the following statements is FALSE:\nPossible Answers\nHis expected odds of getting into medical school improve to 7.8833 (or about 9:8). His expected probability of getting into medical school improves to 88.7%. His expected log-odds of getting into medical school improve by 5.45. His expected probability of getting into medical school improves to 7.9%.  Correct answers on the top of the page\n Graficzna interpretacja modelu df_aug %\u0026gt;% mutate(Survived_hat=round(.fitted)) %\u0026gt;% select(Survived, Survived_hat) %\u0026gt;% table ## Survived_hat ## Survived 0 1 ## 0 462 83 ## 1 219 123 #Out of sample predictions DiCaprio\u0026lt;-data.frame(Fare_log=1) augment(model, newdata = DiCaprio, type.predict = \u0026#39;response\u0026#39;) ## # A tibble: 1 × 2 ## Fare_log .fitted ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.156   Bayesian Statistics - Introduction Introduction The role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update these probability distributions to reflect what has been learned from data.\nLet say I want to set an advertisement on social media. They claim, adds on their surface has 10% of clicks. I a bit sceptical and asses probable efectivnes may range between 0 and 0.20. I assume that binomial model will imitate process generating visitors. Binomial model is my generative model then.\nn_samples \u0026lt;- 100000 n_ads_shown \u0026lt;- 100 proportion_clicks \u0026lt;- runif(n_samples, min = 0.0, max = 0.2) n_visitors \u0026lt;- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks) par(mfrow=c(1,2)) # Visualize proportion clicks hist(proportion_clicks) # Visualize n_visitors hist(n_visitors) Below I present joint distribution over both the underlying proportion of clicks and how many visitors I would get.\nI ran my ad campaign, and 13 people clicked and visited your site when the ad was shown a 100 times. I would now like to use this new information to update the Bayesian model. The reason that we call it posterior is because it represents the uncertainty after (that is, posterior to) having included the information in the data.\n# Create the posterior data frame posterior \u0026lt;- prior[prior$n_visitors == 13, ] # Visualize posterior proportion clicks - below I condition the joint distribution - of prior distribution of proportion_clicks and distribution of n_visitors hist(posterior$proportion_clicks) Now we want to use this updated proportion_clicks to predict how many visitors we would get if we reran the ad campaign.\n# Assign posterior to a new variable called prior prior \u0026lt;- posterior # Take a look at the first rows in prior head(prior) ## proportion_clicks n_visitors ## 29 0.1554544 13 ## 32 0.1233524 13 ## 53 0.1892127 13 ## 73 0.1170808 13 ## 131 0.1713921 13 ## 141 0.0867069 13 # Replace prior$n_visitors with a new sample and visualize the result n_samples \u0026lt;- nrow(prior) n_ads_shown \u0026lt;- 100 prior$n_visitors \u0026lt;- rbinom(n_samples, size = n_ads_shown, prob = prior$proportion_clicks) hist(prior$n_visitors)  Priors Beta distribution The Beta distribution is a useful probability distribution when you want model uncertainty over a parameter bounded between 0 and 1. Here you’ll explore how the two parameters of the Beta distribution determine its shape.\nSo the larger the shape parameters are, the more concentrated the beta distribution becomes.\n# Explore using the rbeta function beta_1 \u0026lt;- rbeta(n = 1000000, shape1 = 1, shape2 = 1) beta_2 \u0026lt;- rbeta(n = 1000000, shape1 = 100, shape2 = 100) beta_3 \u0026lt;- rbeta(n = 1000000, shape1 = 100, shape2 = 20) beta_4 \u0026lt;- rbeta(n = 1000000, shape1 = 5, shape2 = 95) par(mfrow=c(2,2)) hist(beta_1, breaks=seq(0,1,0.02), main = \u0026quot;shape1 = 1, shape2 = 1\u0026quot;) hist(beta_2, breaks=seq(0,1,0.02), main = \u0026quot;shape1 = 100, shape2 = 100\u0026quot;) hist(beta_3, breaks=seq(0,1,0.02), main = \u0026quot;shape1 = 100, shape2 = 20\u0026quot;) hist(beta_4, breaks=seq(0,1,0.02), main = \u0026quot;shape1 = 5, shape2 = 95\u0026quot;) The 4th graphs represents the best following setence: Most ads get clicked on 5% of the time, but for some ads it is as low as 2% and for others as high as 8%.\n  Contrasts and comparison Let say, I initialize also text add campaign, get 6 visitors out of 100 views and now I want to compare which one video or text add is more cost effective.\n# Define parameters n_draws \u0026lt;- 100000 n_ads_shown \u0026lt;- 100 proportion_clicks \u0026lt;- runif(n_draws, min = 0.0, max = 0.2) n_visitors \u0026lt;- rbinom(n = n_draws, size = n_ads_shown, prob = proportion_clicks) prior \u0026lt;- data.frame(proportion_clicks, n_visitors) # Create the posteriors for video and text ads posterior_video \u0026lt;- prior[prior$n_visitors == 13, ] posterior_text \u0026lt;- prior[prior$n_visitors == 6, ] # Visualize the posteriors hist(posterior_video$proportion_clicks, xlim = c(0, 0.25)) hist(posterior_text$proportion_clicks, xlim = c(0, 0.25)) posterior \u0026lt;- data.frame(video_prop = posterior_video$proportion_clicks[1:4000], text_prop = posterior_text$proportion_click[1:4000]) # Calculate the posterior difference: video_prop - text_prop posterior$prop_diff \u0026lt;- posterior$video_prop - posterior$text_prop # Visualize prop_diff hist(posterior$prop_diff) # Calculate the median of prop_diff median(posterior$prop_diff) ## [1] 0.06513359 # Calculate the proportion mean(posterior$prop_diff \u0026gt; 0.0) ## [1] 0.943 #Different adds have differnt costs then: visitor_spend \u0026lt;- 2.53 video_cost \u0026lt;- 0.25 text_cost \u0026lt;- 0.05 # Add the column posterior$video_profit posterior$video_profit \u0026lt;- posterior$video_prop * visitor_spend - video_cost # Add the column posterior$text_profit posterior$text_profit \u0026lt;- posterior$text_prop * visitor_spend - text_cost # Visualize the video_profit and text_profit columns hist(posterior$video_profit) hist(posterior$text_profit) # Add the column posterior$profit_diff posterior$profit_diff \u0026lt;- posterior$video_profit - posterior$text_profit # Visualize posterior$profit_diff hist(posterior$profit_diff) # Calculate a \u0026quot;best guess\u0026quot; for the difference in profits median(posterior$profit_diff) ## [1] -0.03521202 # Calculate the probability that text ads are better than video ads mean(posterior$profit_diff \u0026lt; 0) ## [1] 0.646 #So it seems that the evidence does not strongly favor neither text nor video ads. But if forced to choose the text ads is better. Changeing Generative model Company has changed the way how they price adds. Now they take money just for full day of exposition. Binomial model, which approximate participation of succes in all trials (click in all views) is no longer valid. For new scenario. Poison distribution is now needed.\nThe Poison distribution takes only one parameter which is the mean number of events per time unit\nIn R you can simulate from a Poisson distribution using rpois where lambda is the average number of occurrences:\n# Change the model according to instructions n_draws \u0026lt;- 100000 mean_clicks \u0026lt;- runif(n_draws, min = 0, max = 80) #this is my prior n_visitors \u0026lt;- rpois(n = n_draws, mean_clicks) prior \u0026lt;- data.frame(mean_clicks, n_visitors) posterior \u0026lt;- prior[prior$n_visitors == 19, ] hist(prior$mean_clicks) hist(posterior$mean_clicks)   Dealing with 2 parameter model # the temperatures of Sweden water in 21 th of June in few following year temp \u0026lt;- c(19,23,20,17,23) # Defining the parameter grid - here are are my priors about the posible values of parameters of distribution pars \u0026lt;- expand.grid(mu = seq(8,30, by = 0.5), sigma = seq(0.1, 10, by= 0.3)) # Defining and calculating the prior density for each parameter combination pars$mu_prior \u0026lt;- dnorm(pars$mu, mean = 18, sd = 5) pars$sigma_prior \u0026lt;- dunif(pars$sigma, min = 0, max = 10) pars$prior \u0026lt;- pars$mu_prior * pars$sigma_prior # Calculating the likelihood for each parameter combination for(i in 1:nrow(pars)) { likelihoods \u0026lt;- dnorm(temp, pars$mu[i], pars$sigma[i]) pars$likelihood[i] \u0026lt;- prod(likelihoods) } # Calculate the probability of each parameter combination pars$probability \u0026lt;- pars$likelihood * pars$prior pars$probability \u0026lt;- pars$probability / sum(pars$probability ) library(lattice) levelplot(probability ~ mu * sigma, data = pars) What’s likely the average water temperature for this lake on 20th of Julys, and what’s the probability the water temperature is going to be 18 or more on the next 20th?\nRight now the posterior probability distribution is represented as a data frame with one row per parameter combination with the corresponding probability.\nhead(pars) ## mu sigma mu_prior sigma_prior prior likelihood probability ## 1 8.0 0.1 0.01079819 0.1 0.001079819 0 0 ## 2 8.5 0.1 0.01312316 0.1 0.001312316 0 0 ## 3 9.0 0.1 0.01579003 0.1 0.001579003 0 0 ## 4 9.5 0.1 0.01880982 0.1 0.001880982 0 0 ## 5 10.0 0.1 0.02218417 0.1 0.002218417 0 0 ## 6 10.5 0.1 0.02590352 0.1 0.002590352 0 0 But my questions are much easier to answer if the posterior is represented as a large number of samples, like in earlier chapters. So, let’s draw a sample from this posterior.\nsample_indices \u0026lt;- sample(1:nrow(pars), size=10000, replace=TRUE, prob=pars$probability) pars_sample \u0026lt;- pars[sample_indices,c(\u0026quot;mu\u0026quot;,\u0026quot;sigma\u0026quot;)] head(pars_sample) ## mu sigma ## 476 20.5 3.1 ## 252 21.0 1.6 ## 919 17.0 6.1 ## 524 22.0 3.4 ## 746 20.5 4.9 ## 697 18.5 4.6 What is probabibility of temperature being 18 or above? Not mean temperature, the actual temperature.\n#rnorm is vectorized and implicitly loops over mu and sigma pred_temp\u0026lt;- rnorm(10000, mean=pars_sample$mu, sd=pars_sample$sigma) par(mfrow=c(1,2)) hist(pars_sample$mu,30, main = \u0026#39;probability distribution of mean temperature\u0026#39;) hist(pred_temp,30, main = \u0026#39;probability distribution of tempeture\u0026#39; ) mean(pred_temp\u0026gt;=18) ## [1] 0.7272  Automatisation - BEST package The Bayesian model behind BEST assumes that the generative model for the data is a t-distribution; a more flexible distribution than the normal distribution as it assumes that data points might be outliers to some degree. This makes BEST’s estimate of the mean difference robust to outliers in the data.\nThe t-distribution is just like the normal distribution, a generative model with a mean and a standard deviation that generates heap shaped data. The difference is that the t-distribution has an extra parameter, sometimes called the degrees-of-freedom parameter, that governs how likely the t-distribution is to generate outliers far from its center.\nAnother way in which BEST is different is that BEST uses a so-called Markov chain Monte Carlo method to fit the model. Markov chain Monte Carlo, or MCMC for short, returns a table of samples from the posterior, we can work with the output just like before.\n# The IQ of zombies on a regular diet and a brain based diet. iq_brains \u0026lt;- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51) iq_regular \u0026lt;- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46) # Calculate the mean difference in IQ between the two groups mean(iq_brains) - mean(iq_regular) # Fit the BEST model to the data from both groups library(BEST) library(rjags) best_posterior \u0026lt;- BESTmcmc(iq_brains, iq_regular) # Plot the model result plot(best_posterior) Assume that a super smart mutant zombie (IQ = 150) got into the iq_regular group by mistake. This might mess up the results as you and your colleagues really were interested in how diet affects normal zombies.\n# The IQ of zombies given a regular diet and a brain based diet. iq_brains \u0026lt;- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51) iq_regular \u0026lt;- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 150) # \u0026lt;- Mutant zombie # Modify the data above and calculate the difference in means mean(iq_brains) - mean(iq_regular) # Fit the BEST model to the modified data and plot the result library(BEST) best_posterior \u0026lt;- BESTmcmc(iq_brains, iq_regular) plot(best_posterior)  Conclusions Bayes allows you to tweak, change and tinker with the model to better fit the data analytical problem you have. But a last reason to use Bayes is because it is optimal, kind of. It can be shown, theoretically, that no other method learns as efficiently from data as Bayesian inference.\nIn above examples I show what Bayesian model is about: * I describe my expectations of proportion_clicks as uniform distribution (prior) * Then i describe a generative model which will be responsible for generating views based on proportion_clicks - the second source of variability. For this aim I use two diffrent distribution - binomial and poison - depending on specifity of exercise. * I was able to say which add wass better, more, I was able to say which add was better in probability way.\n  Bayesian Statistics - Intermediate Likelihood On the example of poll. Imagine I am taking part in election to local goverment. Based on many historical election poles I can count on 45% of votes. Votes chances are approximate by bheta function.\ndf\u0026lt;-data.frame(sample=seq(0,1,0.01), density=dbeta(x=seq(0,1,0.01),shape1=45,shape2=55)) df %\u0026gt;% ggplot(aes(x=sample,y=density))+ geom_line()+ ggtitle(\u0026quot;Density function\u0026quot;) Lets imagine that i receive 60% of votes in ellection pole. I can assume that binomial distribution is well suited for generative model responsible for how many votes I am geting. Then I may ask myself: **How probable would be obtaining such a results (60%) of votes under different succes_rate (paramter of Binomial distribution).\ndf\u0026lt;-data.frame(likelihood=dbinom(x=6,size=10,prob=seq(0,1,0.1)), parameter_p=seq(0,1,0.1)) df %\u0026gt;% ggplot(aes(x=parameter_p,y=likelihood))+ geom_line()+ ggtitle(\u0026quot;Likelihood distribution over different succes_rate parameters\u0026quot;) The likelihood function summarizes the likelihood of observing polling data X under different values of the underlying support parameter p. Thus, the likelihood is a function of p that depends upon the observed data X\n Posterior Since I’ve got the prior \u0026amp; likelihood:\n prior: let say based on the historical pole % of votes I can count on is described by betha distribution Betha(45.55) –\u0026gt; most probable is geting 45% votes\n likelihood: is denoting to the most recent data shown above\n  I can approach now to modeling posterior model of p According to Bayes rules posterior is calculating by:\nposterior = prior * likelihood\nHowever, in more sophisticated model settings, tidy, closed-form solutions to this formula might not exist. Very loosely speaking, the goal here is to send information out to the JAGS program, which will then design an algorithm to sample from the posterior, based on which I will then simulate the posterior.\nCompiling rjags model Built from previous polls \u0026amp; election data, my prior model of is a Beta(,) with shape parameters a=45 and b=55. For added insight into p, I also polled potential voters. The dependence of X, the number of these voters that support you, on p is modeled by the Bin(n,p) distribution.\nIn the completed poll, X=6 of n=10 voters supported you. The next goal is to update my model of in light of these observed polling data! To this end, I will use the rjags package to approximate the posterior model of . This exercise will be break down into the 3 rjags steps: define, compile, simulate.\nlibrary(rjags) # DEFINE the model vote_model \u0026lt;- \u0026quot;model{ # Likelihood model for X X ~ dbin(p, n) # Prior model for p p ~ dbeta(a, b) }\u0026quot; # COMPILE the model vote_jags \u0026lt;- jags.model(textConnection(vote_model), data = list(a = 45, b = 55, X = 6, n = 10), inits = list(.RNG.name = \u0026quot;base::Wichmann-Hill\u0026quot;, .RNG.seed = 100)) # SIMULATE the posterior vote_sim \u0026lt;- coda.samples(model = vote_jags, variable.names = c(\u0026quot;p\u0026quot;), n.iter = 10000) # PLOT the posterior plot(vote_sim, trace = FALSE)    "
},
{
	"uri": "https://rombix92.github.io/blog/projects/2020-12-01-r-rmarkdown/",
	"title": "Grow of Metropolis",
	"tags": ["python", "geocoding", "assesment center"],
	"description": "Algorith showing which cities will be absorbed by which metropolis based on Metropolis power and distance to cities.",
	"content": "  Sys.setenv(RETICULATE_PYTHON = \u0026quot;/Users/lrabalski1/miniforge3/envs/everyday_use/bin/python\u0026quot;) reticulate::py_config() ## python: /Users/lrabalski1/miniforge3/envs/everyday_use/bin/python ## libpython: /Users/lrabalski1/miniforge3/envs/everyday_use/lib/libpython3.8.dylib ## pythonhome: /Users/lrabalski1/miniforge3/envs/everyday_use:/Users/lrabalski1/miniforge3/envs/everyday_use ## version: 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:21:17) [Clang 11.1.0 ] ## numpy: /Users/lrabalski1/miniforge3/envs/everyday_use/lib/python3.8/site-packages/numpy ## numpy_version: 1.21.4 ## ## NOTE: Python version was forced by RETICULATE_PYTHON import pandas as pd import numpy as np #!pip install mpu --user import mpu import geopy.distance def radius(population): METRO_CITY_POPULATION_CONSTANT = -1/1443000 MIN_METRO_CITY_RADIUS = 10 MAX_METRO_CITY_RADIUS = 100 - MIN_METRO_CITY_RADIUS return MIN_METRO_CITY_RADIUS + MAX_METRO_CITY_RADIUS * (1 - np.exp(METRO_CITY_POPULATION_CONSTANT * population)) def _calcualate_metrocity_impact(max_radius, distance_to_metro_city): METRO_CITY_POWER_CONSTANT = -1.4 impact = np.exp(METRO_CITY_POWER_CONSTANT * distance_to_metro_city / max_radius) return impact #https://towardsdatascience.com/heres-how-to-calculate-distance-between-2-geolocations-in-python-93ecab5bbba4 #funkcja do liczenia odleglosci miedzy dwiema wspołrzednymi def haversine_distance_code(lat1, lon1, lat2, lon2): r = 6371 phi1 = np.radians(lat1) phi2 = np.radians(lat2) delta_phi = np.radians(lat2 - lat1) delta_lambda = np.radians(lon2 - lon1) a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2 res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))) return np.round(res, 2) df= pd.read_csv(\u0026#39;/Users/lrabalski1/Desktop/prv/x_predict/PL/PL copy.csv\u0026#39;, sep=\u0026quot;\\t\u0026quot;, names=[\u0026#39;geonameid\u0026#39;,\u0026#39;name\u0026#39;,\u0026#39;asciiname\u0026#39;,\u0026#39;alternatenames\u0026#39;,\u0026#39;latitude\u0026#39;,\u0026#39;longitude\u0026#39;,\u0026#39;feature class\u0026#39;,\u0026#39;feature code\u0026#39;,\u0026#39;country code\u0026#39;,\u0026#39;cc2\u0026#39;,\u0026#39;admin1 code\u0026#39;,\u0026#39;admin2 code\u0026#39;,\u0026#39;admin3 code\u0026#39;,\u0026#39;admin4 code\u0026#39;,\u0026#39;population\u0026#39;,\u0026#39;elevation\u0026#39;,\u0026#39;dem\u0026#39;,\u0026#39;timezone\u0026#39;,\u0026#39;modification date\u0026#39;,]) df ## geonameid name ... timezone modification date ## 0 462259 Zodenen ... Europe/Warsaw 2015-09-05 ## 1 477032 Variazhanka ... Europe/Warsaw 2021-08-04 ## 2 490932 Solokiya ... Europe/Warsaw 2021-02-07 ## 3 502656 Rata ... Europe/Warsaw 2014-07-08 ## 4 558461 Hrodzyenskaye Uzvyshsha ... Europe/Warsaw 2010-09-15 ## ... ... ... ... ... ... ## 57943 12324160 WA2 - Parzniew ... Europe/Warsaw 2021-09-16 ## 57944 12324253 Polanki ... Europe/Warsaw 2021-09-22 ## 57945 12324489 Tunezeal Headquarters ... Europe/Warsaw 2021-10-04 ## 57946 12358469 Zębice ... Europe/Warsaw 2021-10-12 ## 57947 12358470 Sulęcin ... Europe/Warsaw 2021-10-12 ## ## [57948 rows x 19 columns] z readme wynika: feature classes: A: country, state, region,… H: stream, lake, … L: parks,area, … P: city, village,… R: road, railroad S: spot, building, farm T: mountain,hill,rock,… U: undersea V: forest,heath,… ### interesuje nas P oraz byc może A\nimport requests url = \u0026#39;http://www.geonames.org/export/codes.html\u0026#39; html = requests.get(url).content df_list = pd.read_html(html) df_legend = df_list[-1] df_legend = df_legend.rename(columns={df_legend.columns[0]: \u0026#39;feature code\u0026#39;, df_legend.columns[1]: \u0026#39;short descr\u0026#39;, df_legend.columns[2]: \u0026#39;long descr\u0026#39;}) df_legend = pd.merge(df[[\u0026#39;feature code\u0026#39;,\u0026#39;feature class\u0026#39;]].drop_duplicates(),df_legend, on=\u0026#39;feature code\u0026#39;) df_legend ## feature code ... long descr ## 0 PPLQ ... NaN ## 1 STM ... a body of running water moving to a lower leve... ## 2 HLLS ... rounded elevations of limited extent rising ab... ## 3 CNL ... an artificial watercourse ## 4 PPL ... a city, town, village, or other agglomeration ... ## .. ... ... ... ## 186 BCN ... a fixed artificial navigation mark ## 187 HSEC ... a large house, mansion, or chateau, on a large... ## 188 RES ... a tract of public land reserved for future use... ## 189 STNR ... a facility for producing and transmitting info... ## 190 BLDA ... a building containing several individual apart... ## ## [191 rows x 4 columns] df = df[df[\u0026#39;feature class\u0026#39;].isin([\u0026#39;P\u0026#39;,\u0026#39;A\u0026#39;])] df_check = pd.merge(df,df_legend, on=[\u0026#39;feature code\u0026#39;,\u0026#39;feature class\u0026#39;]) #sorujac po najwiekszych obiektach widac ze mamy doczynienia z miastami df_check[df_check[\u0026#39;feature class\u0026#39;]==\u0026#39;P\u0026#39;].sort_values(\u0026#39;population\u0026#39;, ascending=False) #nazwy jednostek administracyjnych ktore znajduja sie w zbiorze obiektow P ## geonameid ... long descr ## 47339 756135 ... NaN ## 47350 3093133 ... seat of a first-order administrative division ... ## 47351 3094802 ... seat of a first-order administrative division ... ## 47346 3081368 ... seat of a first-order administrative division ... ## 47348 3088171 ... seat of a first-order administrative division ... ## ... ... ... ... ## 16321 772147 ... a city, town, village, or other agglomeration ... ## 16322 772148 ... a city, town, village, or other agglomeration ... ## 16323 772149 ... a city, town, village, or other agglomeration ... ## 16324 772150 ... a city, town, village, or other agglomeration ... ## 50396 12111088 ... a populated place that no longer exists ## ## [47413 rows x 21 columns] df_check[df_check[\u0026#39;feature class\u0026#39;]==\u0026#39;A\u0026#39;].sort_values(\u0026#39;population\u0026#39;, ascending=False).head(50) #z tej tabeli wynika, ze PPLX to sekcje zaludnionych miejsc, sa to ulice, dzielnice, wiec wykluczam, sa czescia miast ## geonameid ... long descr ## 47378 798544 ... NaN ## 47381 858787 ... a primary administrative division of a country... ## 47305 756136 ... an administrative division of a country, undif... ## 47391 3337497 ... a primary administrative division of a country... ## 47392 3337498 ... a primary administrative division of a country... ## 47380 858786 ... a primary administrative division of a country... ## 47386 3337492 ... a primary administrative division of a country... ## 47328 3093134 ... an administrative division of a country, undif... ## 47387 3337493 ... a primary administrative division of a country... ## 47390 3337496 ... a primary administrative division of a country... ## 47335 3099435 ... an administrative division of a country, undif... ## 47379 858785 ... a primary administrative division of a country... ## 47382 858788 ... a primary administrative division of a country... ## 47394 3337500 ... a primary administrative division of a country... ## 47337 3102015 ... an administrative division of a country, undif... ## 47416 6695624 ... a subdivision of a first-order administrative ... ## 48624 7531926 ... a subdivision of a second-order administrative... ## 47393 3337499 ... a primary administrative division of a country... ## 47311 763168 ... an administrative division of a country, undif... ## 47385 858791 ... a primary administrative division of a country... ## 47316 769251 ... an administrative division of a country, undif... ## 47384 858790 ... a primary administrative division of a country... ## 47318 776070 ... an administrative division of a country, undif... ## 47383 858789 ... a primary administrative division of a country... ## 47389 3337495 ... a primary administrative division of a country... ## 47388 3337494 ... a primary administrative division of a country... ## 47403 6690154 ... a subdivision of a first-order administrative ... ## 48491 7531791 ... a subdivision of a second-order administrative... ## 47417 6697536 ... a subdivision of a first-order administrative ... ## 48454 7531754 ... a subdivision of a second-order administrative... ## 47492 7530801 ... a subdivision of a first-order administrative ... ## 47995 7531292 ... a subdivision of a second-order administrative... ## 48536 7531836 ... a subdivision of a second-order administrative... ## 47549 7530858 ... a subdivision of a first-order administrative ... ## 48589 7531890 ... a subdivision of a second-order administrative... ## 47693 7531002 ... a subdivision of a first-order administrative ... ## 48466 7531766 ... a subdivision of a second-order administrative... ## 47531 7530840 ... a subdivision of a first-order administrative ... ## 47685 7530994 ... a subdivision of a first-order administrative ... ## 48081 7531378 ... a subdivision of a second-order administrative... ## 47505 7530814 ... a subdivision of a first-order administrative ... ## 49563 7532869 ... a subdivision of a second-order administrative... ## 47661 7530970 ... a subdivision of a first-order administrative ... ## 48293 7531591 ... a subdivision of a second-order administrative... ## 47463 7530772 ... a subdivision of a first-order administrative ... ## 47483 7530792 ... a subdivision of a first-order administrative ... ## 48731 7532033 ... a subdivision of a second-order administrative... ## 47459 7530768 ... a subdivision of a first-order administrative ... ## 48309 7531607 ... a subdivision of a second-order administrative... ## 47634 7530943 ... a subdivision of a first-order administrative ... ## ## [50 rows x 21 columns] df_check[[\u0026#39;feature class\u0026#39;,\u0026#39;feature code\u0026#39;, \u0026#39;short descr\u0026#39;]].drop_duplicates() #finalnie musze skupic sie na na obu klasach, jednoczesnie usuwajac duplikaty ## feature class ... short descr ## 0 P ... abandoned populated place ## 142 P ... populated place ## 43484 P ... section of populated place ## 45716 P ... seat of a third-order administrative division ## 47126 P ... seat of a second-order administrative division ## 47304 A ... administrative division ## 47339 P ... capital of a political entity ## 47340 P ... seat of a first-order administrative division ## 47356 P ... farm village ## 47362 P ... populated locality ## 47378 A ... independent political entity ## 47379 A ... first-order administrative division ## 47395 A ... second-order administrative division ## 47775 A ... historical fourth-order administrative division ## 47780 P ... NaN ## 47781 A ... third-order administrative division ## 50259 P ... populated places ## 50262 P ... religious populated place ## 50263 A ... historical third-order administrative division ## 50264 A ... fourth-order administrative division ## 50332 P ... seat of a fourth-order administrative division ## 50333 P ... historical populated place ## ## [22 rows x 3 columns] df = df[(df[\u0026#39;feature class\u0026#39;].isin([\u0026#39;P\u0026#39;])) \u0026amp; (df.population != 0) \u0026amp; ~(df[\u0026#39;feature code\u0026#39;].isin([\u0026#39;PPLX\u0026#39;]))].drop_duplicates(\u0026#39;name\u0026#39;) df.groupby(\u0026#39;feature code\u0026#39;).apply(lambda x: x.sample(1)).reset_index(drop=True) ## geonameid name ... timezone modification date ## 0 3083966 Świniary Wielkie ... Europe/Warsaw 2010-10-16 ## 1 763166 Olsztyn ... Europe/Warsaw 2019-09-05 ## 2 3084237 Sulęcin ... Europe/Warsaw 2020-04-22 ## 3 3088749 Pobiedziska ... Europe/Warsaw 2020-04-25 ## 4 756135 Warsaw ... Europe/Warsaw 2019-11-04 ## 5 766060 Łoje ... Europe/Warsaw 2014-10-02 ## ## [6 rows x 19 columns] df.index.name = \u0026#39;city_id\u0026#39; df.reset_index(inplace=True) df.groupby([\u0026#39;feature class\u0026#39;,\u0026#39;feature code\u0026#39;]).agg({\u0026#39;population\u0026#39;: [\u0026#39;mean\u0026#39;, \u0026#39;min\u0026#39;, \u0026#39;max\u0026#39;]}) ## population ## mean min max ## feature class feature code ## P PPL 3.238169e+03 5 244969 ## PPLA 3.652322e+05 118433 768755 ## PPLA2 3.543081e+04 5696 226794 ## PPLA3 5.619329e+03 110 248125 ## PPLC 1.702139e+06 1702139 1702139 ## PPLF 1.750000e+02 175 175  Metropolie w Polsce (wikipedia: https://pl.wikipedia.org/wiki/Obszar_metropolitalny) Warszawa, Katowice, Kraków, Łódź, Trójmiasto, Poznań, Wrocław, Bydgoszcz, Szczecin, Lublin.\ndf_metropolie = df[df.name.isin( [\u0026#39;Warsaw\u0026#39;,\u0026#39;Katowice\u0026#39;,\u0026#39;Kraków\u0026#39;,\u0026#39;Łódź\u0026#39;, \u0026#39;Gdańsk\u0026#39;,\u0026#39;Gdynia\u0026#39;,#\u0026#39;Trójmiasto\u0026#39;, \u0026#39;Poznań\u0026#39;,\u0026#39;Wrocław\u0026#39;,\u0026#39;Bydgoszcz\u0026#39;,\u0026#39;Szczecin\u0026#39;,\u0026#39;Lublin\u0026#39;])][ [\u0026#39;city_id\u0026#39;,\u0026#39;name\u0026#39;,\u0026#39;population\u0026#39;,\u0026#39;latitude\u0026#39;,\u0026#39;longitude\u0026#39;]] df_metropolie[\u0026#39;iteration\u0026#39;]=0 #df_metropolie[\u0026#39;radius\u0026#39;] = radius(df_metropolie[\u0026#39;population\u0026#39;]) df_metropolie=df_metropolie.add_suffix(\u0026#39;_metro\u0026#39;) df_metropolie ## city_id_metro name_metro ... longitude_metro iteration_metro ## 188 3191 Warsaw ... 21.01178 0 ## 917 12873 Lublin ... 22.56667 0 ## 1734 25287 Wrocław ... 17.03333 0 ## 1916 27732 Szczecin ... 14.55302 0 ## 2279 32047 Poznań ... 16.92993 0 ## 2654 36976 Łódź ... 19.47395 0 ## 2774 38634 Kraków ... 19.93658 0 ## 2889 40299 Katowice ... 19.02754 0 ## 3089 43232 Gdynia ... 18.53188 0 ## 3090 43241 Gdańsk ... 18.64912 0 ## 3298 45802 Bydgoszcz ... 18.00762 0 ## ## [11 rows x 6 columns] #algorytm przypisywania metropolii ## Instrukcja 0. stworze id kolumne z indeksem 1. zlacze tabele z metropoliami i wszystkimi miastami im do tej pory przypisanymi, wylicze zagregowana ludnosc oraz promien metropoli 2. croos joinuje do kazdego miasta bez przypisanej metropolii tabele z metropolia 2. wylicze odleglosc miejscowosci od metropoli i pozbede sie tych wierszy ktore sa poza promieniem 3. dla pozostalych miejscowosci wylicze moc metropolii 4. zrobie slice max groupujac po id miejscowosci pozostawiajc metropolie wchlaniajaca - tak powstanie tabela incrementalna do ktorej potem bede rbindowal nastepne tego typu tabele 5. w obu tabelach powstanie tabele z indeksem mowiacy o n-iteracji z jakiej pochodzi przypisanie miejscowosci do metropolii oraz stan populacji\nwszystko zamkne w lupie while ktory bedzie wykonywany tak dlugo jak zostanie odnotowany przyrost w tabeli incrementalnej  df_cities = df[[\u0026#39;city_id\u0026#39;,\u0026#39;name\u0026#39;,\u0026#39;population\u0026#39;,\u0026#39;latitude\u0026#39;,\u0026#39;longitude\u0026#39;]] df_cities = df_cities.loc[~df_cities.city_id.isin(df_metropolie.city_id_metro)] df_cities ## city_id name population latitude longitude ## 0 13 Prędocin 536 51.14791 21.32704 ## 1 16 Poraj 266 50.89962 23.99191 ## 2 37 Żyrzyn 1400 51.49918 22.09170 ## 3 41 Żyrardów 41179 52.04880 20.44599 ## 4 42 Żyraków 1400 50.08545 21.39622 ## ... ... ... ... ... ... ## 3571 55679 Gądki 529 52.31202 17.04696 ## 3572 55894 Stare Kaleńsko 333 53.52572 16.15265 ## 3573 57439 Lipczynek 40 53.88156 17.25829 ## 3574 57590 Wola Bykowska 200 51.45739 19.65062 ## 3575 57591 Krajanów 160 50.59751 16.44484 ## ## [3565 rows x 5 columns]  wlasciwy algorytm df_miasta_w_puli =df_cities column_names = [\u0026#39;city_id\u0026#39;,\u0026#39;name\u0026#39;,\u0026#39;population\u0026#39;] +df_metropolie.columns.values.tolist() df_miasta_wchloniete=pd.DataFrame(columns=column_names) start = True iteration =0 # start funkcji while start == True: df_metropolie_powiekszone=df_metropolie.append(df_miasta_wchloniete, ignore_index=True) df_metropolie_powiekszone.population = df_metropolie_powiekszone.population.combine_first(df_metropolie_powiekszone.population_metro) df_metropolie_powiekszone_popul = df_metropolie_powiekszone.groupby( [\u0026#39;city_id_metro\u0026#39;,\u0026#39;name_metro\u0026#39;,\u0026#39;population_metro\u0026#39;,\u0026#39;latitude_metro\u0026#39;,\u0026#39;longitude_metro\u0026#39;,]).agg( {\u0026#39;population\u0026#39;:[\u0026#39;sum\u0026#39;]}).reset_index() df_metropolie_powiekszone_popul.columns = df_metropolie_powiekszone_popul.columns.droplevel(1) df_metropolie_powiekszone_popul[\u0026#39;radius\u0026#39;] = radius(df_metropolie_powiekszone_popul[\u0026#39;population\u0026#39;]) df_miasta_w_puli[\u0026#39;key\u0026#39;] = 1 df_metropolie_powiekszone_popul[\u0026#39;key\u0026#39;] = 1 df_x = pd.merge(df_miasta_w_puli, df_metropolie_powiekszone_popul, on=\u0026#39;key\u0026#39;, suffixes=(\u0026#39;\u0026#39;,\u0026#39;_y\u0026#39;)).drop(\u0026quot;key\u0026quot;, 1) #calculating distance between two coordinates #https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude distances_km = [] for row in df_x.itertuples(): distances_km.append( haversine_distance_code( row.latitude, row.longitude ,row.latitude_metro, row.longitude_metro) ) df_x[\u0026#39;distance_km\u0026#39;] = distances_km df_x = df_x[df_x.radius \u0026gt;= df_x.distance_km] df_x[\u0026#39;impact\u0026#39;] = _calcualate_metrocity_impact(df_x.radius,df_x.distance_km) #stwierdzam do ktorej finalnie metropoli miejscowosci zostaje zaliczon idx = df_x.groupby([\u0026#39;name\u0026#39;,\u0026#39;population\u0026#39;])[\u0026#39;impact\u0026#39;].transform(max) == df_x[\u0026#39;impact\u0026#39;] df_x = df_x[idx] iteration+= 1 df_x[\u0026#39;iteration_metro\u0026#39;]=iteration pre_rows_num=df_miasta_wchloniete.shape[0] df_miasta_wchloniete=df_miasta_wchloniete.append( df_x[column_names], ignore_index=True) #pozbywam sie miast juz wchlonietych indx = df_miasta_w_puli.city_id.isin(df_miasta_wchloniete.city_id) df_miasta_w_puli = df_miasta_w_puli[~indx] if pre_rows_num == df_miasta_wchloniete.shape[0]: start = False ## \u0026lt;string\u0026gt;:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument \u0026#39;labels\u0026#39; will be keyword-only ## \u0026lt;string\u0026gt;:10: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df_metropolie_powiekszone_popul = df_metropolie_powiekszone.groupby( [\u0026#39;city_id_metro\u0026#39;,\u0026#39;name_metro\u0026#39;,\u0026#39;population_metro\u0026#39;,\u0026#39;latitude_metro\u0026#39;,\u0026#39;longitude_metro\u0026#39;,]).agg( {\u0026#39;population\u0026#39;:[\u0026#39;sum\u0026#39;]}).reset_index() df_metropolie_powiekszone_popul.columns = df_metropolie_powiekszone_popul.columns.droplevel(1) df_metropolie_powiekszone_popul[\u0026#39;radius\u0026#39;] = radius(df_metropolie_powiekszone_popul[\u0026#39;population\u0026#39;])  #finalne populacje metropoli df_metropolie_powiekszone_popul #przypisanie miast do metropoli wraz numerem iteracji ## city_id_metro name_metro ... population radius ## 0 3191 Warsaw ... 5167905 97.494601 ## 1 12873 Lublin ... 531712 37.739146 ## 2 25287 Wrocław ... 1038518 56.178876 ## 3 27732 Szczecin ... 589846 40.197589 ## 4 32047 Poznań ... 1116528 58.484992 ## 5 36976 Łódź ... 1522125 68.657469 ## 6 38634 Kraków ... 1624389 70.801803 ## 7 40299 Katowice ... 4402773 95.742488 ## 8 43232 Gdynia ... 445607 33.910908 ## 9 43241 Gdańsk ... 785065 47.764670 ## 10 45802 Bydgoszcz ... 1011566 55.352705 ## ## [11 rows x 7 columns] df_miasta_wchloniete ## city_id name ... longitude_metro iteration_metro ## 0 41 Żyrardów ... 21.01178 1 ## 1 189 Zręczyce ... 19.93658 1 ## 2 215 Żoliborz ... 21.01178 1 ## 3 291 Złota ... 21.01178 1 ## 4 339 Zielonki-Wieś ... 21.01178 1 ## ... ... ... ... ... ... ## 1781 41015 Jeżewo ... 18.00762 6 ## 1782 44601 Dąbrowa Biskupia ... 18.00762 6 ## 1783 47909 Aleksandrów Kujawski ... 18.00762 6 ## 1784 32645 Płużnica ... 18.00762 7 ## 1785 38419 Kruszwica ... 18.00762 7 ## ## [1786 rows x 9 columns]  "
},
{
	"uri": "https://rombix92.github.io/blog/encyklopedia/",
	"title": "Encyklopedia",
	"tags": [],
	"description": "",
	"content": "   R_Markdown  Presentation of R Markdown functionality\n Data Exploration  Methods for data explorations\n Statistics  Describing statistical methods\n "
},
{
	"uri": "https://rombix92.github.io/blog/projects/",
	"title": "Projects",
	"tags": [],
	"description": "",
	"content": "   Grow of Metropolis  Algorith showing which cities will be absorbed by which metropolis based on Metropolis power and distance to cities.\n "
},
{
	"uri": "https://rombix92.github.io/blog/tags/assesment-center/",
	"title": "assesment center",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://rombix92.github.io/blog/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://rombix92.github.io/blog/tags/geocoding/",
	"title": "geocoding",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://rombix92.github.io/blog/",
	"title": "Lukasz Rabalski website",
	"tags": [],
	"description": "",
	"content": "  Lukasz Rabalski website  "
},
{
	"uri": "https://rombix92.github.io/blog/tags/python/",
	"title": "python",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://rombix92.github.io/blog/categories/r/",
	"title": "R",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://rombix92.github.io/blog/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]